{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax-Regression with ridge-regulation and early stopping.\n",
    "    \"\"\"\n",
    "    def __init__(self, eta: float=0.1, max_iter: int=5000, epsilon: float=1e-7, alpha: float=0.1, stop_at_first_minimum: bool=False, validation_size: float=0.2):\n",
    "        \"\"\"\n",
    "        Instantiate softmax regression\n",
    "        Args:\n",
    "            eta: learning rate (step-size). Default 0.1\n",
    "            max_iter: the maximum number of training epochs. Default 5000\n",
    "            epsilon: tiny value which is added to the result of the softmax function while calculating the loss to avoid nan (Not a Number) values\n",
    "            alpha: regulatoin-weight for the ridge-regulation (l2-norm)\n",
    "            validation_size: size of the validation set. Default 0.2\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.validation_size = validation_size\n",
    "        self.stop_at_first_minimum = stop_at_first_minimum\n",
    "        pass\n",
    "    \n",
    "    def _calc_score(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the softmax-scores\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset with bias term\n",
    "        \"\"\"\n",
    "        return X.dot(self.Theta)\n",
    "    \n",
    "    def _calc_softmax(self, scores: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Estimate the probabilities for each point and category using the softmax function.\n",
    "        \n",
    "        Args:\n",
    "            scores: softmax-scores\n",
    "        \"\"\"\n",
    "        exps = np.exp(scores)\n",
    "        exps_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "        return exps / exps_sums\n",
    "\n",
    "    def _calc_loss(self, X: np.ndarray, y: np.ndarray, y_proba: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the loss with the cross entropy as loss function. A l2-regulation is added.\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset with bias term\n",
    "            y: labels (one-hot encoded)\n",
    "            y_proba: probabilities for each point and category (from self._calc_softmax)\n",
    "\n",
    "        Returns:\n",
    "            Loss\n",
    "        \"\"\"\n",
    "        xentropy_loss = -np.mean(np.sum(y * np.log(y_proba + self.epsilon), axis=1))\n",
    "        l2_loss = 0.5 * np.sum(np.square(self.Theta[1:]))\n",
    "        return xentropy_loss + self.alpha * l2_loss\n",
    "    \n",
    "    def _calc_gradients(self, X: np.ndarray, y: np.ndarray, y_proba: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculates the gradients.\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset with bias term\n",
    "            y: labels (one-hot encoded)\n",
    "            y_proba: probabilities for each point and category (from self._calc_softmax)\n",
    "            \n",
    "        Returns:\n",
    "            The gradients\n",
    "        \"\"\"\n",
    "        error = y_proba - y\n",
    "        return 1/self.m * X.T.dot(error) + np.r_[np.zeros([1, self.n_outputs]), self.alpha * self.Theta[1:]]\n",
    "    \n",
    "    def split(X: np.ndarray, y: np.ndarray, ratio: float=0.2) -> tuple:\n",
    "        \"\"\"\n",
    "        Splits\n",
    "        Args:\n",
    "            X: Dataset\n",
    "            y: labels\n",
    "            ratio: size of the second set (the smaller one). Default 0.2\n",
    "        \n",
    "        Returns:\n",
    "            tuple with X_1, X_2, y_1, y_2\n",
    "        \"\"\"\n",
    "        total_size = len(X)\n",
    "        \n",
    "        size_two = int(total_size * ratio)\n",
    "        size_one = total_size - size_two\n",
    "        \n",
    "        random_indices = np.random.permutation(total_size)\n",
    "        \n",
    "        X_1 = X[random_indices[:size_one]]\n",
    "        y_1 = y[random_indices[:size_one]]\n",
    "        \n",
    "        X_2 = X[random_indices[size_one:]]\n",
    "        y_2 = y[random_indices[size_one:]]\n",
    "        return X_1, X_2, y_1, y_2\n",
    "    \n",
    "    def transform(self, X: np.ndarray, y=None):\n",
    "        \"\"\"\n",
    "        Encode the labels with one-hot and adds bias term to the X\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset (without bias term)\n",
    "            y: labels (NOT one-hot encoded). Default None\n",
    "        \n",
    "        Returns:\n",
    "            tuple with X with bias term and a one-hot encoded y or only X if y is None\n",
    "        \"\"\"\n",
    "        X = np.c_[np.ones([len(X), 1]), X]\n",
    "        if y is not None:\n",
    "            cats = [y==cat for cat in range(y.max() + 1)]\n",
    "            y = np.array(cats).astype(np.int).T\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predicts categories for dataset\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset with bias term\n",
    "        \"\"\"\n",
    "        return self._calc_softmax(self._calc_score(X))\n",
    "    \n",
    "    def fit_once(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: Dataset with bias term\n",
    "            y: Labels (one-hot encoded)\n",
    "            \n",
    "        Returns:\n",
    "            Loss\n",
    "        \"\"\"\n",
    "        self.n_inputs = X.shape[1]\n",
    "        self.n_outputs = y.shape[1]\n",
    "        self.m = len(X)\n",
    "        \n",
    "        self.Theta = np.random.randn(self.n_inputs, self.n_outputs)\n",
    "        y_proba = self.predict(X)\n",
    "        loss = self._calc_loss(X, y, y_proba)\n",
    "        gradients = self._calc_gradients(X, y, y_proba)\n",
    "        \n",
    "        self.Theta = self.Theta - self.eta * gradients\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Fits the model with early stopping and l2-regulation\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset with bias term\n",
    "            y: Labels (one-hot encoded)\n",
    "        \"\"\"\n",
    "        X_train, X_val, y_train, y_val = Softmax.split(X, y, self.validation_size)\n",
    "        \n",
    "        self.best_loss = np.infty\n",
    "        self.best_Theta = None\n",
    "        self.best_epoch = None\n",
    "        \n",
    "        for epoch in range(self.max_iter):\n",
    "            self.fit_once(X_train, y_train)\n",
    "            validation_loss = self._calc_loss(X_val, y_val, self.predict(X_val))\n",
    "            if validation_loss < self.best_loss:\n",
    "                self.best_loss = validation_loss\n",
    "                self.best_Theta = deepcopy(self.Theta)\n",
    "                self.best_epoch = epoch\n",
    "            else:\n",
    "                if self.stop_at_first_minimum:\n",
    "                    break\n",
    "        if self.best_Theta is not None:\n",
    "            self.Theta = deepcopy(self.best_Theta)\n",
    "    \n",
    "    def fit_transform(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Encodes the labels with one-hot and fits the model.\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset (without bias term)\n",
    "            y: labels (NOT one-hot encoded)\n",
    "        \"\"\"\n",
    "        X, y = self.transform(X, y)\n",
    "        self.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "a = iris['data'][:, (2,3)]\n",
    "b = iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = Softmax.split(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Softmax(max_iter=10000, stop_at_first_minimum=False)\n",
    "clf.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = np.argmax(clf.predict(clf.transform(X_test)), axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_test)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3012"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7172113734524296"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.3644425 , -0.83455237, -0.7295832 ],\n",
       "       [-0.44403057,  0.51176669,  0.19469608],\n",
       "       [-0.21475755, -0.20064712,  0.46387196]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_Theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
