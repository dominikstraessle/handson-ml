{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"\n",
    "    Softmax-Regression with ridge-regulation, early stopping and mini-batch option.\n",
    "    \"\"\"\n",
    "    def __init__(self, eta: float=0.1, max_iter: int=5000, epsilon: float=1e-1, alpha: float=0.1, stop_at_first_minimum: bool=False, validation_size: float=0.2, batch_size: int=64):\n",
    "        \"\"\"\n",
    "        Instantiate softmax regression\n",
    "        Args:\n",
    "            eta: learning rate (step-size). Default 0.1\n",
    "            max_iter: the maximum number of training epochs. Default 5000\n",
    "            epsilon: tiny value which is added to the result of the softmax function while calculating the loss to avoid nan (Not a Number) values\n",
    "            alpha: regulatoin-weight for the ridge-regulation (l2-norm)\n",
    "            validation_size: size of the validation set. Default 0.2\n",
    "            batch_size: Size of a batch (mini-batch learning). Default 512. If None normal batch-learning is used.\n",
    "        \"\"\"\n",
    "        self.eta = eta\n",
    "        self.max_iter = max_iter\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.validation_size = validation_size\n",
    "        self.stop_at_first_minimum = stop_at_first_minimum\n",
    "        self.batch_size = batch_size\n",
    "        pass\n",
    "    \n",
    "    def _calc_score(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculate the softmax-scores\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset with bias term\n",
    "        \"\"\"\n",
    "        return X.dot(self.Theta)\n",
    "    \n",
    "    def _calc_softmax(self, scores: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Estimate the probabilities for each point and category using the softmax function.\n",
    "        \n",
    "        Args:\n",
    "            scores: softmax-scores\n",
    "        \"\"\"\n",
    "        exps = np.exp(scores)\n",
    "        exps_sums = np.sum(exps, axis=1, keepdims=True)\n",
    "        return exps / exps_sums\n",
    "\n",
    "    def _calc_loss(self, X: np.ndarray, y: np.ndarray, y_proba: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the loss with the cross entropy as loss function. A l2-regulation is added.\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset with bias term\n",
    "            y: labels (one-hot encoded)\n",
    "            y_proba: probabilities for each point and category (from self._calc_softmax)\n",
    "\n",
    "        Returns:\n",
    "            Loss\n",
    "        \"\"\"\n",
    "        xentropy_loss = -np.mean(np.sum(y * np.log(y_proba + self.epsilon), axis=1))\n",
    "        l2_loss = 0.5 * np.sum(np.square(self.Theta[1:]))\n",
    "        return xentropy_loss + self.alpha * l2_loss\n",
    "    \n",
    "    def _calc_gradients(self, X: np.ndarray, y: np.ndarray, y_proba: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Calculates the gradients.\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset with bias term\n",
    "            y: labels (one-hot encoded)\n",
    "            y_proba: probabilities for each point and category (from self._calc_softmax)\n",
    "            \n",
    "        Returns:\n",
    "            The gradients\n",
    "        \"\"\"\n",
    "        error = y_proba - y\n",
    "        return 1/self.m * X.T.dot(error) + np.r_[np.zeros([1, self.n_outputs]), self.alpha * self.Theta[1:]]\n",
    "    \n",
    "    def split(X: np.ndarray, y: np.ndarray, ratio: float=0.2) -> tuple:\n",
    "        \"\"\"\n",
    "        Splits\n",
    "        Args:\n",
    "            X: Dataset\n",
    "            y: labels\n",
    "            ratio: size of the second set (the smaller one). Default 0.2\n",
    "        \n",
    "        Returns:\n",
    "            tuple with X_1, X_2, y_1, y_2\n",
    "        \"\"\"\n",
    "        total_size = len(X)\n",
    "        \n",
    "        size_two = int(total_size * ratio)\n",
    "        size_one = total_size - size_two\n",
    "        \n",
    "        if size_two == 0:\n",
    "            raise ValueError(F'More entries are required, currently: {total_size}')\n",
    "        \n",
    "        random_indices = np.random.permutation(total_size)\n",
    "        \n",
    "        X_1 = X[random_indices[:size_one]]\n",
    "        y_1 = y[random_indices[:size_one]]\n",
    "        \n",
    "        X_2 = X[random_indices[size_one:]]\n",
    "        y_2 = y[random_indices[size_one:]]\n",
    "        return X_1, X_2, y_1, y_2\n",
    "    \n",
    "    def transform(self, X: np.ndarray, y=None):\n",
    "        \"\"\"\n",
    "        Encode the labels with one-hot and adds bias term to the X\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset (without bias term)\n",
    "            y: labels (NOT one-hot encoded). Default None\n",
    "        \n",
    "        Returns:\n",
    "            tuple with normalized X with bias term and a one-hot encoded y or only X if y is None\n",
    "        \"\"\"\n",
    "        X = np.c_[np.ones([len(X), 1]), X]\n",
    "        \n",
    "        # normalize all features\n",
    "        X /= X.max()\n",
    "        \n",
    "        # one-hot encoding\n",
    "        if y is not None:\n",
    "            cats = [y==cat for cat in range(y.max() + 1)]\n",
    "            y = np.array(cats).astype(np.int).T\n",
    "            return X, y\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Predicts categories for dataset\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset with bias term\n",
    "        \"\"\"\n",
    "        return self._calc_softmax(self._calc_score(X))\n",
    "    \n",
    "    def fit_once(self, X: np.ndarray, y: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X: Dataset with bias term\n",
    "            y: Labels (one-hot encoded)\n",
    "            \n",
    "        Returns:\n",
    "            Loss\n",
    "        \"\"\"\n",
    "        self.n_inputs = X.shape[1]\n",
    "        self.n_outputs = y.shape[1]\n",
    "        self.m = len(X)\n",
    "        \n",
    "        if self.batch_size is not None and self.batch_size < self.m:\n",
    "            batch_indices = np.random.randint(0, self.m-1, self.batch_size)\n",
    "            X = X[batch_indices]\n",
    "            y = y[batch_indices]\n",
    "        \n",
    "        self.Theta = np.random.randn(self.n_inputs, self.n_outputs)\n",
    "        y_proba = self.predict(X)\n",
    "        loss = self._calc_loss(X, y, y_proba)\n",
    "        gradients = self._calc_gradients(X, y, y_proba)\n",
    "        \n",
    "        self.Theta = self.Theta - self.eta * gradients\n",
    "        return loss\n",
    "    \n",
    "    def fit(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Fits the model with early stopping and l2-regulation\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset with bias term\n",
    "            y: Labels (one-hot encoded)\n",
    "        \"\"\"\n",
    "        X_train, X_val, y_train, y_val = Softmax.split(X, y, self.validation_size)\n",
    "        \n",
    "        self.best_loss = np.infty\n",
    "        self.best_Theta = None\n",
    "        self.best_epoch = None\n",
    "        \n",
    "        for epoch in range(self.max_iter):\n",
    "            loss = self.fit_once(X_train, y_train)\n",
    "            y_val_proba = self.predict(X_val)\n",
    "            validation_loss = self._calc_loss(X_val, y_val, y_val_proba)\n",
    "            if epoch % int(self.max_iter / 10) == 0:\n",
    "                print(F'Train Lss: {loss}')\n",
    "                print(F'Val. Loss: {validation_loss}')\n",
    "                print(F'Best Loss: {self.best_loss}')\n",
    "            if validation_loss < self.best_loss:\n",
    "                self.best_loss = validation_loss\n",
    "                self.best_Theta = deepcopy(self.Theta)\n",
    "                self.best_epoch = epoch\n",
    "            else:\n",
    "                if self.stop_at_first_minimum and self.best_loss is not np.infty:\n",
    "                    break\n",
    "        if self.best_Theta is not None:\n",
    "            self.Theta = deepcopy(self.best_Theta)\n",
    "            print(F'Best Loss: {self.best_loss}')\n",
    "\n",
    "    \n",
    "    def fit_transform(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Encodes the labels with one-hot, normalize X and fits the model.\n",
    "        \n",
    "        Args:\n",
    "            X: Dataset (without bias term)\n",
    "            y: labels (NOT one-hot encoded)\n",
    "        \"\"\"\n",
    "        X, y = self.transform(X, y)\n",
    "        self.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "X_test = X_test.reshape((X_test.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Lss: 390.5148243820718\n",
      "Val. Loss: 1.6060772909554093\n",
      "Best Loss: inf\n",
      "Train Lss: 393.16511742411495\n",
      "Val. Loss: 1.6046119646228836\n",
      "Best Loss: 1.598465210164543\n",
      "Train Lss: 395.17991401953446\n",
      "Val. Loss: 1.603690934060315\n",
      "Best Loss: 1.598465210164543\n",
      "Train Lss: 386.72892016795015\n",
      "Val. Loss: 1.6036743063424879\n",
      "Best Loss: 1.598465210164543\n",
      "Train Lss: 396.3813853231682\n",
      "Val. Loss: 1.6052236419955723\n",
      "Best Loss: 1.598465210164543\n",
      "Train Lss: 397.10733081122726\n",
      "Val. Loss: 1.603599164524404\n",
      "Best Loss: 1.598465210164543\n",
      "Train Lss: 402.7100451375413\n",
      "Val. Loss: 1.605833758859314\n",
      "Best Loss: 1.598465210164543\n",
      "Train Lss: 386.27641615735575\n",
      "Val. Loss: 1.6041308580606866\n",
      "Best Loss: 1.598465210164543\n",
      "Train Lss: 397.73796947656444\n",
      "Val. Loss: 1.6066126036752406\n",
      "Best Loss: 1.598465210164543\n",
      "Train Lss: 400.93564322661547\n",
      "Val. Loss: 1.6036931815929996\n",
      "Best Loss: 1.598465210164543\n",
      "Best Loss: 1.598465210164543\n"
     ]
    }
   ],
   "source": [
    "clf = Softmax(max_iter=2500, stop_at_first_minimum=False, eta=10, alpha=0.1, batch_size=50)\n",
    "clf.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = np.argmax(clf.predict(clf.transform(X_test[:100])), axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_test[:100])\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.598465210164543"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mit iris - schon besser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[\"data\"][:, 3:]  # petal width\n",
    "y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = Softmax.split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Lss: 0.5298486035576471\n",
      "Val. Loss: 0.5089781651458902\n",
      "Best Loss: inf\n",
      "Best Loss: 0.5089781651458902\n"
     ]
    }
   ],
   "source": [
    "clf = Softmax(max_iter=5000, stop_at_first_minimum=True, batch_size=None)\n",
    "clf.fit_transform(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9666666666666667"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_predict = np.argmax(clf.predict(clf.transform(X_test)), axis=1)\n",
    "\n",
    "accuracy_score = np.mean(y_predict == y_test)\n",
    "accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5089781651458902"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert clf.Theta.all() == clf.best_Theta.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
